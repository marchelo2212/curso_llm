{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marchelo2212/curso_llm/blob/main/notebook/EST_Notebook_4_Chatbot_FAQ_QA_RAG_Memory_save_embedding_VF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hVhTJtwmvBda",
      "metadata": {
        "id": "hVhTJtwmvBda"
      },
      "source": [
        "# Notebook 4 — Chatbot RAG con memoria (con retrieval y citas) con persistencia de base de datos vectorial (embedding)\n",
        "\n",
        "Pipeline: **PyPDFLoader → RecursiveCharacterTextSplitter → OpenAIEmbeddings → Chroma → RetrievalQA**\n",
        "\n",
        "**Objetivo:**\n",
        "\n",
        "- Construir un Pipeline de Indexación RAG: Demostrar cómo procesar documentos PDF (PyPDFLoader), dividirlos en fragmentos (RecursiveCharacterTextSplitter) y convertirlos en vectores (HuggingFaceEmbeddings) almacenados en una base de datos vectorial (Chroma).\n",
        "\n",
        "- Implementar Búsqueda Semántica: Enseñar a construir un retriever que encuentre los fragmentos más relevantes basándose en el significado (vectores), como solución a la fragilidad de la búsqueda léxica (difflib) del notebook anterior.\n",
        "\n",
        "- Demostrar \"Grounding\" y Citas: Construir una cadena RAG (LCEL) que fuerce al LLM a basar sus respuestas únicamente en los fragmentos de texto recuperados ({context}) y muestre las fuentes (citas) de donde provino la información.\n",
        "\n",
        "- Comparar Estrategias de Búsqueda: Introducir y comparar dos métodos de retrieval: la búsqueda por similitud estándar (similarity) y la búsqueda por Relevancia Marginal Máxima (mmr) para obtener resultados más diversos.\n",
        "\n",
        "- Crear una UI Interactiva Completa: Usar gradio (gr.Blocks) para construir una aplicación web que permita al usuario cargar sus propios PDFs, procesarlos y chatear con ellos, incluyendo un selector para comparar los dos tipos de RAG.\n",
        "\n",
        "> Requisitos: una API key de OpenAI (`OPENAI_API_KEY`) y 1–3 PDFs cortos del curso para pruebas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E6mlo-uOvBdi",
      "metadata": {
        "id": "E6mlo-uOvBdi"
      },
      "source": [
        "## 1) Inslacación de dependencias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fnv7wNdP6xYL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnv7wNdP6xYL",
        "outputId": "d4c403db-95fb-4029-a26b-02b65ab745af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain version instalada: 1.1.0\n"
          ]
        }
      ],
      "source": [
        "# ⬇️ 1. Instalar dependencias\n",
        "# (Instala/actualiza todas las bibliotecas necesarias en un solo comando)\n",
        "!pip install -qU \\\n",
        "    langchain \\\n",
        "    langchain-openai \\\n",
        "    langchain-community \\\n",
        "    langchain-text-splitters \\\n",
        "    chromadb \\\n",
        "    tiktoken \\\n",
        "    pypdf \\\n",
        "    gradio \\\n",
        "    sentence-transformers\n",
        "\n",
        "# ⬇️ 2. Importar y verificar\n",
        "import langchain\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "# --- 1. Importar todas las bibliotecas necesarias ---\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "# ---------------------------------\n",
        "\n",
        "# Imprime la versión instalada para confirmar que se cargó la más reciente\n",
        "print(f\"LangChain version instalada: {langchain.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ZJ_bk7l5pHx-",
      "metadata": {
        "id": "ZJ_bk7l5pHx-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the OPENAI_API_KEY environment variable using the secrets manager\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# --- NUEVO: Montar Google Drive para persistencia ---\n",
        "from google.colab import drive\n",
        "# Esto pedirá autorización al ejecutar la celda\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1BrL-xmTvBdi",
      "metadata": {
        "id": "1BrL-xmTvBdi"
      },
      "source": [
        "## 1) Interfaz rápida con Gradio\n",
        "Aquí una interfaz sencilla para probar preguntas y ver las **citas** a las fuentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gradio",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "gradio",
        "outputId": "bed7d6d8-d5db-480b-f5cd-a3083dfb1484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lanzando interfaz de Gradio...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando procesamiento...\n",
            "✅ ChromaDB inicializado/cargado. Documentos en DB: 120\n",
            "No se subieron archivos nuevos. Intentando cargar DB existente...\n",
            "Documentos en la base de datos: 120\n",
            "✅ Pipeline RAG (Similarity) con memoria listo.\n",
            "✅ Pipeline RAG (MMR) con memoria listo.\n",
            "Ejecutando con: RAG (Similarity) y 0 mensajes en historial.\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from operator import itemgetter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# --- 2. Configuración Global (Modelos) ---\n",
        "\n",
        "# Carga el modelo de embedding gratuito\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\" #NO CAMBIAR\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name) #NO CAMBIAR\n",
        "\n",
        "# Carga el LLM (asume que la API key de OpenAI está en el entorno)\n",
        "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "\n",
        "# Plantilla de Prompt (usaremos la misma para ambos RAGs para una comparación justa)\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"Responde la pregunta de forma concisa (máximo 3 frases) basándote solo en este contexto:\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Pregunta: {input}\"\"\")\n",
        "\n",
        "# Directorio de persistencia en Google Drive\n",
        "persist_directory = \"/content/drive/MyDrive/chroma_hf\"\n",
        "\n",
        "# --- Función para limpiar la base de datos ---\n",
        "def clear_database():\n",
        "    import shutil\n",
        "    if os.path.exists(persist_directory):\n",
        "        shutil.rmtree(persist_directory)\n",
        "        return \"✅ Base de datos ChromaDB limpiada.\"\n",
        "    return \"⚠️ La base de datos no existe o ya está vacía.\"\n",
        "\n",
        "\n",
        "# --- 3. Función de Indexación (El \"Paso 1\") ---\n",
        "# Esta función AHORA CREA AMBAS CADENAS RAG y maneja PERSISTENCIA\n",
        "\n",
        "def process_files(file_list):\n",
        "    print(\"Iniciando procesamiento...\")\n",
        "\n",
        "    vectordb = None\n",
        "    collection_count = 0\n",
        "    msg_extra = \"\"\n",
        "\n",
        "    # Intenta cargar ChromaDB existente o crear una nueva.\n",
        "    # Maneja específicamente la incompatibilidad de dimensiones.\n",
        "    try:\n",
        "        # Intenta inicializar ChromaDB. Esto intentará cargar los datos existentes\n",
        "        # si persist_directory existe y es compatible.\n",
        "        vectordb = Chroma(\n",
        "            persist_directory=persist_directory,\n",
        "            embedding_function=embeddings\n",
        "        )\n",
        "        collection_count = vectordb._collection.count()\n",
        "        print(f\"✅ ChromaDB inicializado/cargado. Documentos en DB: {collection_count}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if \"Collection expecting embedding with dimension\" in str(e):\n",
        "            print(f\"⚠️ Se encontró una ChromaDB incompatible en {persist_directory} (dimensiones no coinciden).\")\n",
        "            # Intenta limpiar la base de datos incompatible\n",
        "            if os.path.exists(persist_directory):\n",
        "                print(f\"  Intentando limpiar el directorio: {persist_directory}\")\n",
        "                try:\n",
        "                    import shutil\n",
        "                    shutil.rmtree(persist_directory)\n",
        "                    print(\"  ✅ Directorio de ChromaDB incompatible limpiado.\")\n",
        "                    msg_extra = \"⚠️ Base de datos incompatible encontrada y eliminada. \"\n",
        "                except Exception as rmtree_e:\n",
        "                    print(f\"  ❌ Error al limpiar el directorio: {rmtree_e}\")\n",
        "                    return None, f\"❌ Error al limpiar base de datos incompatible: {rmtree_e}\"\n",
        "            else:\n",
        "                print(f\"  ⚠️ El directorio {persist_directory} no se encontró a pesar del error de dimensiones. Esto es inesperado.\")\n",
        "                return None, f\"❌ Error: Base de datos incompatible reportada, pero el directorio no existe. Por favor, comprueba tu Google Drive.\"\n",
        "\n",
        "            # Después de limpiar (o si no existía pero el error persistió debido a algún estado interno de Chroma),\n",
        "            # re-inicializa Chroma para crear una *nueva* base de datos compatible.\n",
        "            print(\"  Re-inicializando ChromaDB para crear una nueva colección compatible.\")\n",
        "            vectordb = Chroma(\n",
        "                persist_directory=persist_directory,\n",
        "                embedding_function=embeddings\n",
        "            )\n",
        "            collection_count = vectordb._collection.count() # Debería ser 0\n",
        "            print(f\"  ✅ Nueva ChromaDB inicializada. Documentos en DB: {collection_count}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"❌ Error al inicializar ChromaDB: {e}\")\n",
        "            return None, f\"❌ Error al inicializar ChromaDB: {e}\"\n",
        "\n",
        "    # Si vectordb sigue siendo None en este punto, algo salió muy mal,\n",
        "    # aunque el try-except anterior debería asegurar que siempre se inicialice.\n",
        "    if vectordb is None:\n",
        "        return None, \"❌ Error interno: No se pudo inicializar ChromaDB.\"\n",
        "\n",
        "\n",
        "    # Si el usuario subió archivos, los procesamos y AÑADIMOS a la DB existente (o recién creada)\n",
        "    if file_list:\n",
        "        print(f\"Procesando {len(file_list)} archivos nuevos...\")\n",
        "        all_chunks = []\n",
        "        try:\n",
        "            for file_obj in file_list:\n",
        "                print(f\"  Cargando: {os.path.basename(file_obj.name)}\")\n",
        "                loader = PyPDFLoader(file_obj.name)\n",
        "                docs = loader.load()\n",
        "                splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=120)\n",
        "                chunks = splitter.split_documents(docs)\n",
        "                all_chunks.extend(chunks)\n",
        "\n",
        "            print(f\"Total de chunks generados: {len(all_chunks)}\")\n",
        "\n",
        "            # Añadir a la DB existente (incremental)\n",
        "            print(\"Añadiendo nuevos documentos a ChromaDB...\")\n",
        "            vectordb.add_documents(documents=all_chunks)\n",
        "            # Chroma 0.4+ persiste automáticamente\n",
        "\n",
        "            msg_extra += f\"✅ Se añadieron {len(all_chunks)} nuevos fragmentos.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            err = str(e)\n",
        "            if \"Collection expecting embedding with dimension\" in err:\n",
        "                print(\"⚠️ Incompatibilidad de dimensiones detectada al añadir documentos.\")\n",
        "                msg = clear_database()\n",
        "                print(msg)\n",
        "                return None, (\n",
        "                    \"⚠️ Se detectó una base de datos Chroma creada con otro modelo de embedding. \"\n",
        "                    \"La he eliminado. Vuelve a ejecutar la celda para reindexar tus PDFs.\"\n",
        "                )\n",
        "\n",
        "            print(f\"Error procesando archivos: {e}\")\n",
        "            return None, f\"❌ Error al procesar archivos: {e}\"\n",
        "    else:\n",
        "        print(\"No se subieron archivos nuevos. Intentando cargar DB existente...\")\n",
        "        if not msg_extra: # Solo añade si no hubo un mensaje previo sobre limpiar la DB\n",
        "            msg_extra = \"✅ Usando base de datos existente en Drive.\"\n",
        "\n",
        "    # Verifica si la DB tiene datos DESPUÉS de procesar los nuevos archivos\n",
        "    # Vuelve a obtener collection_count para reflejar los documentos recién añadidos\n",
        "    collection_count = vectordb._collection.count()\n",
        "    print(f\"Documentos en la base de datos: {collection_count}\")\n",
        "\n",
        "\n",
        "    if collection_count == 0 and not file_list: # Solo si no se subieron archivos Y la colección está vacía\n",
        "        return None, \"⚠️ La base de datos está vacía y no se subieron archivos. Por favor sube PDFs.\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        # --- ¡AQUÍ EMPIEZAN LOS CAMBIOS! ---\n",
        "\n",
        "        # 3. NUEVO: Definir un prompt para re-escribir la consulta\n",
        "        rephrase_prompt = ChatPromptTemplate.from_messages([\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            (\"user\", \"{input}\"),\n",
        "            (\"user\", \"Dada la conversación anterior, genera una consulta de búsqueda (query) independiente para poder encontrar la información y responder a la última pregunta.\")\n",
        "        ])\n",
        "\n",
        "        # 4. NUEVO: Crear una \"cadena de re-escritura\"\n",
        "        # Esta cadena toma el historial y la nueva pregunta, y genera una\n",
        "        # consulta de búsqueda optimizada (un string).\n",
        "        query_rewriter_chain = rephrase_prompt | llm | StrOutputParser()\n",
        "\n",
        "        # 5. NUEVO: Definir el prompt de respuesta (que ahora acepta historial)\n",
        "        # (El 'prompt' anterior no aceptaba historial, por eso es necesario redefinirlo)\n",
        "        answer_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"Responde la pregunta del usuario de forma concisa (máximo 3 frases) basándote solo en el siguiente contexto:\\n<context>\\n{context}\\n</context>\"),\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"), # <-- El historial va aquí\n",
        "            (\"user\", \"{input}\")\n",
        "        ])\n",
        "\n",
        "        # 6. MODIFICADO: Crear el Pipeline RAG (Similarity)\n",
        "        retriever_sim = vectordb.as_retriever(search_type='similarity', search_kwargs={'k': 4})\n",
        "\n",
        "        qa_sim = (\n",
        "            {\n",
        "                # El 'context' ahora se recupera usando la consulta RE-ESCRITA\n",
        "                \"context\": RunnablePassthrough.assign(\n",
        "                    standalone_query=query_rewriter_chain # 1. Re-escribe la consulta\n",
        "                ) | itemgetter(\"standalone_query\") | retriever_sim, # 2. Usa esa consulta para el retriever\n",
        "\n",
        "                # Pasamos el 'input' (pregunta original) y el 'historial')\n",
        "                \"input\": itemgetter(\"input\"),\n",
        "                \"chat_history\": itemgetter(\"chat_history\")\n",
        "            }\n",
        "            | RunnablePassthrough.assign(\n",
        "                answer=(answer_prompt | llm) # El prompt de respuesta ahora recibe el historial\n",
        "            )\n",
        "        )\n",
        "        print(\"✅ Pipeline RAG (Similarity) con memoria listo.\")\n",
        "\n",
        "        # 7. MODIFICADO: Crear el Pipeline RAG (MMR)\n",
        "        retriever_mmr = vectordb.as_retriever(search_type='mmr', search_kwargs={'k': 4, 'lambda_mult': 0.5})\n",
        "\n",
        "        qa_mmr = (\n",
        "             {\n",
        "                \"context\": RunnablePassthrough.assign(\n",
        "                    standalone_query=query_rewriter_chain # 1. Es la misma cadena de re-escritura (creada para similarity)\n",
        "                ) | itemgetter(\"standalone_query\") | retriever_mmr, # 2. Usa el retriever MMR\n",
        "\n",
        "                \"input\": itemgetter(\"input\"),\n",
        "                \"chat_history\": itemgetter(\"chat_history\")\n",
        "            }\n",
        "            | RunnablePassthrough.assign(\n",
        "                answer=(answer_prompt | llm) # Usa el mismo prompt de respuesta de similarity\n",
        "            )\n",
        "        )\n",
        "        print(\"✅ Pipeline RAG (MMR) con memoria listo.\")\n",
        "\n",
        "        # 8. (Igual que antes) Guardar AMBAS -cuando no tenía memoria- cadenas en un diccionario\n",
        "        all_chains = {\n",
        "            \"RAG (Similarity)\": qa_sim,\n",
        "            \"RAG (MMR)\": qa_mmr\n",
        "        }\n",
        "\n",
        "        return all_chains, f\"✅ ¡Éxito! Base de datos cargada ({collection_count} docs). {msg_extra}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando archivos durante la creación de cadenas RAG: {e}\")\n",
        "        return None, f\"❌ Error: {e}\"\n",
        "\n",
        "\n",
        "def rag_chat(user_message, history, rag_chain_state, rag_type):\n",
        "\n",
        "    # Comprueba si el pipeline RAG (\"state\") ha sido creado\n",
        "    if rag_chain_state is None:\n",
        "        return \"Por favor, sube uno o más archivos PDF y presiona 'Procesar Archivos' primero.\"\n",
        "\n",
        "    # --- Convertir el historial de Gradio al formato de LangChain ---\n",
        "    chat_history_messages = []\n",
        "\n",
        "    if not history:\n",
        "        chat_history_messages = []\n",
        "    else:\n",
        "        first = history[0]\n",
        "\n",
        "        # Caso 1: historia como lista de pares (user, bot) o listas de 2+\n",
        "        if isinstance(first, (list, tuple)):\n",
        "            for turn in history:\n",
        "                # si vienen más de 2 campos, nos quedamos con los dos primeros\n",
        "                if len(turn) >= 2:\n",
        "                    human_msg = turn[0]\n",
        "                    ai_msg = turn[1]\n",
        "                    if human_msg is not None and human_msg != \"\":\n",
        "                        chat_history_messages.append(HumanMessage(content=str(human_msg)))\n",
        "                    if ai_msg is not None and ai_msg != \"\":\n",
        "                        chat_history_messages.append(AIMessage(content=str(ai_msg)))\n",
        "\n",
        "        # Caso 2: historia como lista de dicts tipo {\"role\": \"...\", \"content\": \"...\"}\n",
        "        elif isinstance(first, dict):\n",
        "            for msg in history:\n",
        "                role = msg.get(\"role\")\n",
        "                content = msg.get(\"content\", \"\")\n",
        "                if not content:\n",
        "                    continue\n",
        "                if role == \"user\":\n",
        "                    chat_history_messages.append(HumanMessage(content=content))\n",
        "                elif role in (\"assistant\", \"ai\", \"bot\"):\n",
        "                    chat_history_messages.append(AIMessage(content=content))\n",
        "\n",
        "        # Caso 3: fallback (por si Gradio devuelve algo raro)\n",
        "        else:\n",
        "            # interpretamos como alternancia user/bot en una lista simple\n",
        "            for i, msg in enumerate(history):\n",
        "                content = str(msg)\n",
        "                if i % 2 == 0:\n",
        "                    chat_history_messages.append(HumanMessage(content=content))\n",
        "                else:\n",
        "                    chat_history_messages.append(AIMessage(content=content))\n",
        "    # -----------------------------------------------------------------\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "\n",
        "    # Usa el 'rag_type' para seleccionar la cadena correcta\n",
        "    try:\n",
        "        chain_to_use = rag_chain_state[rag_type]\n",
        "        print(f\"Ejecutando con: {rag_type} y {len(chat_history_messages)} mensajes en historial.\")\n",
        "\n",
        "        # --- MODIFICADO: Invoca la cadena con 'input' Y 'chat_history' ---\n",
        "        out = chain_to_use.invoke({\n",
        "            'input': user_message,\n",
        "            'chat_history': chat_history_messages\n",
        "        })\n",
        "\n",
        "        # (El resto de la función es igual: extraer 'answer' y 'context')\n",
        "        ans = out['answer'].content\n",
        "        srcs = out.get('context', [])\n",
        "\n",
        "        cites = []\n",
        "        for d in srcs:\n",
        "            meta = d.metadata or {}\n",
        "            src = os.path.basename(meta.get('source','?'))\n",
        "            page = meta.get('page','?')\n",
        "            cites.append(f\"{src} (pág. {page})\")\n",
        "\n",
        "        footer = '\\n\\nFuentes:\\n- ' + '\\n- '.join(dict.fromkeys(cites)) if cites else ''\n",
        "        return ans + footer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en 'rag_chat': {e}\")\n",
        "        return f\"Error al generar respuesta: {e}\"\n",
        "\n",
        "\n",
        "# --- 5. Construir la Interfaz de Gradio con \"Blocks\" ---\n",
        "\n",
        "with gr.Blocks(title=\"Chatbot RAG con Carga y Selector\") as demo:\n",
        "\n",
        "    # 'gr.State' es la variable \"invisible\" que guarda el diccionario de pipelines RAG\n",
        "    rag_chain_state = gr.State(None)\n",
        "\n",
        "    gr.Markdown(\"# Chatbot RAG: Carga de Archivos y Comparador (Similarity vs. MMR)\")\n",
        "    gr.Markdown(\"Sube tus PDFs, presiona 'Procesar Archivos' y luego chatea con tus documentos.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # Componente de carga de archivos\n",
        "            file_uploader = gr.File(\n",
        "                label=\"Sube tus PDFs\",\n",
        "                file_count=\"multiple\",\n",
        "                file_types=[\".pdf\"]\n",
        "            )\n",
        "\n",
        "            # ¡NUEVO! Selector de tipo de RAG\n",
        "            rag_selector = gr.Radio(\n",
        "                [\"RAG (Similarity)\", \"RAG (MMR)\"],\n",
        "                label=\"Elige el tipo de Retriever\",\n",
        "                value=\"RAG (Similarity)\" # Valor por defecto\n",
        "            )\n",
        "\n",
        "            # Botón para iniciar la indexación\n",
        "            process_button = gr.Button(\"Procesar Archivos\", variant=\"primary\")\n",
        "\n",
        "            # Botón para limpiar la base de datos (NUEVO)\n",
        "            clear_db_button = gr.Button(\"Limpiar Base de Datos (ChromaDB)\", variant=\"secondary\")\n",
        "\n",
        "            # Caja de estado para mensajes de éxito o error\n",
        "            status_box = gr.Textbox(label=\"Estado\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            # Interfaz de chat estándar\n",
        "            chat_interface = gr.ChatInterface(\n",
        "                fn=rag_chat,\n",
        "                # Pasa AMBAS entradas adicionales a la función de chat\n",
        "                additional_inputs=[rag_chain_state, rag_selector]\n",
        "            )\n",
        "\n",
        "    # --- 6. Conectar los Componentes ---\n",
        "\n",
        "    # Conecta el botón 'process_button' a la función 'process_files'\n",
        "    process_button.click(\n",
        "        fn=process_files,\n",
        "        inputs=[file_uploader],\n",
        "        outputs=[rag_chain_state, status_box] # La salida se guarda en el estado y el status\n",
        "    )\n",
        "\n",
        "    # Conecta el botón 'clear_db_button' a la función 'clear_database' (NUEVO)\n",
        "    clear_db_button.click(\n",
        "        fn=clear_database,\n",
        "        inputs=[],\n",
        "        outputs=[status_box]\n",
        "    )\n",
        "\n",
        "print(\"Lanzando interfaz de Gradio...\")\n",
        "demo.launch(share=False, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QEesKSn1vBdi",
      "metadata": {
        "id": "QEesKSn1vBdi"
      },
      "source": [
        "## 8) Buenas prácticas y notas finales\n",
        "- **Citación obligatoria**: siempre devolver fuentes (archivo y página) para confianza y auditoría.\n",
        "- **Evaluación**: verifica que los fragmentos recuperados contienen evidencia suficiente.\n",
        "- **Tamaño de chunk**: prueba 400–900 tokens; solape 10–20%.\n",
        "- **k y search_type**: ajusta según el corpus. `mmr` puede mejorar diversidad.\n",
        "- **Persistencia**: usa `persist_directory` para reutilizar índices.\n",
        "- **Privacidad**: no subas documentos sensibles a Colab sin autorización."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Usar para borrr la base de datos vectorial\n",
        "# --- Función para limpiar la base de datos ---\n",
        "#def clear_database():\n",
        "#    import shutil\n",
        "#    if os.path.exists(persist_directory):\n",
        "#        shutil.rmtree(persist_directory)\n",
        "#        return \"✅ Base de datos ChromaDB limpiada.\"\n",
        "#    return \"⚠️ La base de datos no existe o ya está vacía.\"\n"
      ],
      "metadata": {
        "id": "QdZn2Qy5nSss"
      },
      "id": "QdZn2Qy5nSss",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Usar para borrr la base de datos vectorial\n",
        "#clear_database()\n"
      ],
      "metadata": {
        "id": "OJiBWcj3oK2q"
      },
      "id": "OJiBWcj3oK2q",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}