{"cells":[{"cell_type":"markdown","metadata":{"id":"hVhTJtwmvBda"},"source":["# Notebook 2 â€” Chatbot RAG (con retrieval y citas)\n","\n","Pipeline: **PyPDFLoader â†’ RecursiveCharacterTextSplitter â†’ OpenAIEmbeddings â†’ Chroma â†’ RetrievalQA**\n","\n","**Objetivo:**\n","\n","- Construir un Pipeline de IndexaciÃ³n RAG: Demostrar cÃ³mo procesar documentos PDF (PyPDFLoader), dividirlos en fragmentos (RecursiveCharacterTextSplitter) y convertirlos en vectores (HuggingFaceEmbeddings) almacenados en una base de datos vectorial (Chroma).\n","\n","- Implementar BÃºsqueda SemÃ¡ntica: EnseÃ±ar a construir un retriever que encuentre los fragmentos mÃ¡s relevantes basÃ¡ndose en el significado (vectores), como soluciÃ³n a la fragilidad de la bÃºsqueda lÃ©xica (difflib) del notebook anterior.\n","\n","- Demostrar \"Grounding\" y Citas: Construir una cadena RAG (LCEL) que fuerce al LLM a basar sus respuestas Ãºnicamente en los fragmentos de texto recuperados ({context}) y muestre las fuentes (citas) de donde provino la informaciÃ³n.\n","\n","- Comparar Estrategias de BÃºsqueda: Introducir y comparar dos mÃ©todos de retrieval: la bÃºsqueda por similitud estÃ¡ndar (similarity) y la bÃºsqueda por Relevancia Marginal MÃ¡xima (mmr) para obtener resultados mÃ¡s diversos.\n","\n","- Crear una UI Interactiva Completa: Usar gradio (gr.Blocks) para construir una aplicaciÃ³n web que permita al usuario cargar sus propios PDFs, procesarlos y chatear con ellos, incluyendo un selector para comparar los dos tipos de RAG.\n","\n","> Requisitos: una API key de OpenAI (`OPENAI_API_KEY`) y 1â€“3 PDFs cortos del curso para pruebas."],"id":"hVhTJtwmvBda"},{"cell_type":"markdown","metadata":{"id":"xXlQhkw7vBdd"},"source":["## 0) InstalaciÃ³n y preparaciÃ³n del entorno\n","Instalaremos los paquetes necesarios y configuraremos la clave de API."],"id":"xXlQhkw7vBdd"},{"cell_type":"code","metadata":{"id":"install","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762382465730,"user_tz":300,"elapsed":36526,"user":{"displayName":"Marcelo Sotaminga","userId":"12238593987105802968"}},"outputId":"80a49aff-8064-4947-fb72-40e42c24f443"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.9/469.9 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n","google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n","google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mLangChain version instalada: 1.0.3\n"]}],"source":["# â¬‡ï¸ 1. Instalar dependencias\n","# (Instala/actualiza todas las bibliotecas necesarias en un solo comando)\n","!pip install -qU \\\n","    langchain \\\n","    langchain-openai \\\n","    langchain-community \\\n","    langchain-text-splitters \\\n","    chromadb \\\n","    tiktoken \\\n","    pypdf \\\n","    gradio \\\n","    sentence-transformers\n","\n","# â¬‡ï¸ 2. Importar y verificar\n","import langchain\n","from langchain_core.runnables import RunnablePassthrough\n","\n","# Imprime la versiÃ³n instalada para confirmar que se cargÃ³ la mÃ¡s reciente\n","print(f\"LangChain version instalada: {langchain.__version__}\")"],"id":"install"},{"cell_type":"code","metadata":{"id":"apikey","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762383757833,"user_tz":300,"elapsed":839,"user":{"displayName":"Marcelo Sotaminga","userId":"12238593987105802968"}},"outputId":"caa5b27b-85db-437b-e587-746691b640f3"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["API key cargada desde Secretos (longitud): 164\n"]}],"source":["import os\n","from google.colab import userdata\n","\n","# --- Instrucciones SECRETS DE GOOGLE ---\n","# 1. Haz clic en el icono de la llave (ğŸ”‘) en el panel izquierdo.\n","# 2. Haz clic en 'Agregar un nuevo secreto'.\n","# 3. Nombre (Name): OPENAI_API_KEY\n","# 4. Valor (Value): psk... (pega tu clave aquÃ­)\n","# 5. Activa el interruptor (toggle) para 'Acceso al notebook'.\n","# 6. Â¡Listo! Ahora puedes ejecutar esta celda.\n","\n","# Obtiene la clave de forma segura desde los secretos de Colab\n","OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n","\n","# Asigna la clave al entorno de ejecuciÃ³n\n","os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n","\n","# Verifica que se haya cargado (imprime la longitud, no la clave)\n","if OPENAI_API_KEY:\n","    print('API key cargada desde Secretos (longitud):', len(OPENAI_API_KEY))\n","else:\n","    print('ERROR: No se encontrÃ³ el secreto \"OPENAI_API_KEY\".')"],"id":"apikey"},{"cell_type":"markdown","metadata":{"id":"XdJELfL2vBdf"},"source":["## 1) Cargar documentos PDF (DocumentLoader)\n","Sube 1â€“3 PDFs (guÃ­as, programas, papers). Se extraerÃ¡ su texto y metadatos (archivo y nÃºmero de pÃ¡gina)."],"id":"XdJELfL2vBdf"},{"cell_type":"code","metadata":{"id":"upload","colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"status":"ok","timestamp":1762384599839,"user_tz":300,"elapsed":149093,"user":{"displayName":"Marcelo Sotaminga","userId":"12238593987105802968"}},"outputId":"0c8ed808-3994-4ef7-ffac-dab020a66012"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Por favor, sube uno o mÃ¡s archivos PDF para procesar...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-457fc43b-ef74-41f6-8bea-aa21150325a1\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-457fc43b-ef74-41f6-8bea-aa21150325a1\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Lee et al. - 2025 - Research Trends and Challenges in Diagnostic Classification Models Insights from Dynamic Topic Mode.pdf to Lee et al. - 2025 - Research Trends and Challenges in Diagnostic Classification Models Insights from Dynamic Topic Mode.pdf\n","Saving Lee et al. - 2025 - AI-powered Automatic Item Generation for Psychological Tests A Conceptual Framework for an LLM-base.pdf to Lee et al. - 2025 - AI-powered Automatic Item Generation for Psychological Tests A Conceptual Framework for an LLM-base (1).pdf\n","\n","Â¡Ã‰xito! Se cargaron 2 archivo(s):\n","  1. Lee et al. - 2025 - Research Trends and Challenges in Diagnostic Classification Models Insights from Dynamic Topic Mode.pdf\n","['Lee et al. - 2025 - Research Trends and Challenges in Diagnostic Classification Models Insights from Dynamic Topic Mode.pdf', 'Lee et al. - 2025 - AI-powered Automatic Item Generation for Psychological Tests A Conceptual Framework for an LLM-base (1).pdf']\n","  2. Lee et al. - 2025 - AI-powered Automatic Item Generation for Psychological Tests A Conceptual Framework for an LLM-base (1).pdf\n","['Lee et al. - 2025 - Research Trends and Challenges in Diagnostic Classification Models Insights from Dynamic Topic Mode.pdf', 'Lee et al. - 2025 - AI-powered Automatic Item Generation for Psychological Tests A Conceptual Framework for an LLM-base (1).pdf']\n"]}],"source":["from google.colab import files\n","\n","# 1. Da una instrucciÃ³n clara al usuario ANTES de mostrar el botÃ³n\n","print(\"Por favor, sube uno o mÃ¡s archivos PDF para procesar...\")\n","\n","# 2. Abre la ventana de diÃ¡logo para cargar archivos\n","uploaded = files.upload()\n","\n","# 3. Obtiene la lista de nombres de archivo\n","pdf_paths = list(uploaded.keys())\n","\n","# 4. Verifica si se subieron archivos y da retroalimentaciÃ³n\n","if pdf_paths:\n","    print(f\"\\nÂ¡Ã‰xito! Se cargaron {len(pdf_paths)} archivo(s):\")\n","    # Imprime los nombres de los archivos cargados\n","    for i, path in enumerate(pdf_paths, 1):\n","        print(f\"  {i}. {path}\")\n","        print(f\"{pdf_paths}\")\n","else:\n","    print(\"\\nCarga cancelada. No se seleccionÃ³ ningÃºn archivo.\")\n","\n"],"id":"upload"},{"cell_type":"code","metadata":{"id":"load","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762384615376,"user_tz":300,"elapsed":11542,"user":{"displayName":"Marcelo Sotaminga","userId":"12238593987105802968"}},"outputId":"536676eb-6531-4665-817b-d0ace7bf5b61"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Total de pÃ¡ginas cargadas: 63\n"]},{"output_type":"execute_result","data":{"text/plain":["{'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)',\n"," 'creator': 'PTC Arbortext Publishing Engine',\n"," 'creationdate': '2025-05-07T16:18:41+05:30',\n"," 'keywords': 'Diagnosis classification models (DCMs); cognitive diagnosis models (CDMs); dynamic topic modeling (DTM); review; research trends',\n"," 'moddate': '2025-05-07T16:18:41+05:30',\n"," 'title': 'Research Trends and Challenges in Diagnostic Classification Models: Insights from Dynamic Topic Mode',\n"," 'rgid': 'PB:391405685_AS:11431281484469514@1749095249386',\n"," 'source': 'Lee et al. - 2025 - Research Trends and Challenges in Diagnostic Classification Models Insights from Dynamic Topic Mode.pdf',\n"," 'total_pages': 34,\n"," 'page': 0,\n"," 'page_label': '1'}"]},"metadata":{},"execution_count":21}],"source":["# Importa la clase especÃ­fica de LangChain para cargar (leer) archivos PDF.\n","from langchain_community.document_loaders import PyPDFLoader\n","\n","# Inicializa una lista vacÃ­a. 'docs' guardarÃ¡ CADA PÃGINA de TODOS los PDFs\n","# como un objeto 'Document' separado.\n","docs = []\n","\n","# Inicia un bucle que recorre la lista 'pdf_paths' (que contiene los nombres\n","# de los archivos que subiste, ej: ['doc1.pdf', 'doc2.pdf']).\n","for path in pdf_paths:\n","\n","    # 1. Crea una instancia del cargador, apuntando al archivo PDF actual ('path').\n","    loader = PyPDFLoader(path)\n","\n","    # 2. Ejecuta la carga. .load() lee el PDF y devuelve una lista ('d')\n","    #    donde cada elemento es una pÃ¡gina del PDF (objeto Document).\n","    d = loader.load()  # cada pÃ¡gina â†’ Document(text, metadata)\n","\n","    # 3. AÃ±ade las pÃ¡ginas de la lista 'd' a la lista principal 'docs'.\n","    #    (Usamos .extend() para aÃ±adir los elementos de la lista, no la lista en sÃ­).\n","    docs.extend(d)\n","\n","# Imprime un mensaje de confirmaciÃ³n mostrando el nÃºmero total de pÃ¡ginas cargadas.\n","print(f'Total de pÃ¡ginas cargadas: {len(docs)}')\n","\n","# Muestra los metadatos de la primera pÃ¡gina (docs[0]) como inspecciÃ³n.\n","# Los metadatos suelen incluir {'source': 'nombre_del_archivo.pdf', 'page': 0}.\n","# 'if docs else {}' evita un error si la lista 'docs' estÃ¡ vacÃ­a.\n","docs[0].metadata if docs else {}"],"id":"load"},{"cell_type":"markdown","metadata":{"id":"tjyoe4rsvBdg"},"source":["## 2) Chunking (TextSplitter)\n","Dividimos cada pÃ¡gina en **fragmentos** para mejorar la recuperaciÃ³n semÃ¡ntica. Puedes ajustar `chunk_size` y `chunk_overlap`."],"id":"tjyoe4rsvBdg"},{"cell_type":"code","metadata":{"id":"chunking","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762384624145,"user_tz":300,"elapsed":56,"user":{"displayName":"Marcelo Sotaminga","userId":"12238593987105802968"}},"outputId":"f6e498ca-0eec-4442-85c9-d453ee0beb4e"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Chunks generados: 402\n"]},{"output_type":"execute_result","data":{"text/plain":["{'producer': 'PDFlib+PDI 9.2.0 (C++/Win64)',\n"," 'creator': 'PTC Arbortext Publishing Engine',\n"," 'creationdate': '2025-05-07T16:18:41+05:30',\n"," 'keywords': 'Diagnosis classification models (DCMs); cognitive diagnosis models (CDMs); dynamic topic modeling (DTM); review; research trends',\n"," 'moddate': '2025-05-07T16:18:41+05:30',\n"," 'title': 'Research Trends and Challenges in Diagnostic Classification Models: Insights from Dynamic Topic Mode',\n"," 'rgid': 'PB:391405685_AS:11431281484469514@1749095249386',\n"," 'source': 'Lee et al. - 2025 - Research Trends and Challenges in Diagnostic Classification Models Insights from Dynamic Topic Mode.pdf',\n"," 'total_pages': 34,\n"," 'page': 0,\n"," 'page_label': '1'}"]},"metadata":{},"execution_count":22}],"source":["# Importa la clase recomendada por LangChain para dividir texto.\n","# 'Recursive' (recursivo) significa que intentarÃ¡ dividir el texto de forma\n","# inteligente usando una lista de separadores, desde el mÃ¡s grande al mÃ¡s pequeÃ±o.\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","# 1. Crea una instancia del divisor de texto con parÃ¡metros especÃ­ficos.\n","splitter = RecursiveCharacterTextSplitter(\n","\n","    # Define el tamaÃ±o mÃ¡ximo de cada 'chunk' (fragmento) en caracteres.\n","    # 700 es un tamaÃ±o comÃºn, lo suficientemente grande para tener contexto,\n","    # pero lo suficientemente pequeÃ±o para ser preciso en la bÃºsqueda.\n","    chunk_size=700,\n","\n","    # Define cuÃ¡ntos caracteres se solaparÃ¡n entre chunks consecutivos.\n","    # 120 es crucial para evitar que una idea se corte bruscamente\n","    # entre el final de un chunk y el inicio del siguiente.\n","    chunk_overlap=120,\n","\n","    # Define la lista de separadores que el splitter intentarÃ¡ usar, EN ORDEN.\n","    # 1ro intentarÃ¡ dividir por doble salto de lÃ­nea (pÃ¡rrafos).\n","    # Si un chunk sigue siendo muy grande, intentarÃ¡ por salto de lÃ­nea simple, etc.\n","    separators=['\\n\\n', '\\n', '. ', ' ']\n",")\n","\n","#\n","\n","# 2. Ejecuta la divisiÃ³n.\n","# Toma la lista 'docs' (que contiene todas las pÃ¡ginas) y aplica las reglas\n","# de divisiÃ³n para generar la nueva lista de 'chunks'.\n","chunks = splitter.split_documents(docs)\n","\n","# Imprime un recuento de cuÃ¡ntos chunks se generaron en total.\n","# (Es normal que 'len(chunks)' sea mucho mayor que 'len(docs)').\n","print('Chunks generados:', len(chunks))\n","\n","# 3. Inspecciona el primer chunk.\n","# Muestra los metadatos del primer fragmento (chunks[0]).\n","# VerÃ¡s que LangChain ha conservado los metadatos originales\n","# (ej. 'source' y 'page') del documento del que proviene.\n","chunks[0].metadata if chunks else {}"],"id":"chunking"},{"cell_type":"markdown","metadata":{"id":"72_MOZtpvBdh"},"source":["## 3) Embeddings y Vector Store (Chroma)\n","Convertimos cada fragmento en un **embedding** y lo indexamos en **Chroma** para bÃºsqueda por similitud.\n","Puedes cambiar el nombre de la colecciÃ³n y activar persistencia en disco si lo deseas."],"id":"72_MOZtpvBdh"},{"cell_type":"code","metadata":{"id":"chroma","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762384669685,"user_tz":300,"elapsed":35240,"user":{"displayName":"Marcelo Sotaminga","userId":"12238593987105802968"}},"outputId":"d6cefda8-4646-4bf2-bb36-5fa4cb1f111a"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Usando embeddings gratuitos de HuggingFace...\n","Vector DB listo. Documentos indexados: 406\n"]}],"source":["# Importa la clase para usar embeddings gratuitos de HuggingFace.\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","# Importa la clase Chroma, que es nuestra base de datos vectorial.\n","from langchain_community.vectorstores import Chroma\n","\n","# 1. Configura el modelo de embedding gratuito\n","\n","# Define el nombre del modelo gratuito que se descargarÃ¡ (uno popular y ligero).\n","model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n","# Crea la instancia del 'embedder' (incrustador), cargando el modelo gratuito.\n","EMBEDDINGS = HuggingFaceEmbeddings(model_name=model_name)\n","\n","# Imprime un mensaje de estado para confirmar quÃ© embedder se estÃ¡ usando.\n","print(\"Usando embeddings gratuitos de HuggingFace...\")\n","\n","# Define un nombre para la \"colecciÃ³n\" (la tabla) dentro de ChromaDB.\n","COLLECTION_NAME = 'rag_curso'\n","# Define la ruta para guardar la BD (None = solo en memoria, no persistente).\n","PERSIST_DIR = None\n","\n","# 2. Indexa los documentos en la Vector Store\n","#\n","\n","# Este es el paso clave: crea la base de datos vectorial en memoria.\n","vectordb = Chroma.from_documents(\n","    documents=chunks,         # La lista de fragmentos de texto (chunks) a indexar.\n","    embedding=EMBEDDINGS,     # El modelo de embedding a usar (el de HuggingFace).\n","    collection_name=COLLECTION_NAME, # El nombre para esta colecciÃ³n de vectores.\n","    persist_directory=PERSIST_DIR  # DÃ³nde guardar (en este caso, 'None' o memoria).\n",")\n","\n","# Imprime un recuento de cuÃ¡ntos chunks se indexaron exitosamente.\n","print('Vector DB listo. Documentos indexados:', vectordb._collection.count())"],"id":"chroma"},{"cell_type":"markdown","metadata":{"id":"E6mlo-uOvBdi"},"source":["## 4) Crear el Retriever y la cadena RetrievalQA\n","El *retriever* recupera los **k** fragmentos mÃ¡s relevantes. `RetrievalQA` usa esos fragmentos como contexto para el LLM y ademÃ¡s nos devuelve las **fuentes**."],"id":"E6mlo-uOvBdi"},{"cell_type":"code","metadata":{"id":"qa-setup","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762384971591,"user_tz":300,"elapsed":4,"user":{"displayName":"Marcelo Sotaminga","userId":"12238593987105802968"}},"outputId":"d08b4012-a85c-40af-e3db-2bac7fae0bce"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["first={\n","  context: RunnableLambda(itemgetter('input'))\n","           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7c030f484770>, search_kwargs={'k': 4}),\n","  input: RunnableLambda(itemgetter('input'))\n","} middle=[] last=RunnableAssign(mapper={\n","  answer: ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='Responde las preguntas de manera clara.\\n La respuesta debe ser muy bien argumentada, empleando lenguaje tÃ©cnico de mÃ¡ximo 400 caracteres, agrega al menos una referencia a los documentos en formato APA7 y basada\\n en este contexto:\\n\\n<context>\\n{context}\\n</context>\\n\\nQuestion: {input}'), additional_kwargs={})])\n","          | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7c030795c1d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7c030795d0d0>, root_client=<openai.OpenAI object at 0x7c030795f1d0>, root_async_client=<openai.AsyncOpenAI object at 0x7c030795cb90>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n","})\n"]}],"source":["#--- 1. Importaciones de LangChain ---\n","\n","# Importa el conector para los modelos de Chat de OpenAI (ej. GPT-4o).\n","from langchain_openai import ChatOpenAI\n","# Importa la clase para crear plantillas de prompts de chat.\n","from langchain_core.prompts import ChatPromptTemplate\n","# Importa componentes de LCEL (LangChain Expression Language).\n","from langchain_core.runnables import RunnablePassthrough\n","# Importa una funciÃ³n de Python para obtener claves de un diccionario.\n","from operator import itemgetter\n","\n","# --- 2. ConfiguraciÃ³n del LLM ---\n","\n","# Crea una instancia del modelo de lenguaje.\n","llm = ChatOpenAI(\n","    model='gpt-4o-mini',  # Especifica el modelo de OpenAI a usar.\n","    temperature=0         # Fija la creatividad en 0 para respuestas consistentes.\n",")\n","\n","# --- 3. ConfiguraciÃ³n del Retriever ---\n","\n","# Crea un objeto 'retriever' (buscador) a partir de la base de datos vectorial.\n","# (Asume que 'vectordb' fue creada en una celda anterior).\n","retriever = vectordb.as_retriever(\n","    # Define el tipo de bÃºsqueda. 'similarity' es la bÃºsqueda estÃ¡ndar.\n","    search_type='similarity',\n","    # Define cuÃ¡ntos documentos ('k') debe recuperar.\n","    search_kwargs={'k': 4}\n",")\n","\n","# --- 4. DefiniciÃ³n del Prompt (Plantilla de Instrucciones) ---\n","\n","# Define la plantilla de texto que se le enviarÃ¡ al LLM.\n","prompt = ChatPromptTemplate.from_template(\n","\n"," \"\"\"Responde las preguntas de manera clara.\n"," La respuesta debe ser muy bien argumentada, empleando lenguaje tÃ©cnico de mÃ¡ximo 400 caracteres, agrega al menos una referencia a los documentos en formato APA7 y basada\n"," en este contexto:\n","\n","<context>\n","{context}\n","</context>\n","\n","Question: {input}\"\"\")\n","\n","# Esta es la forma moderna y correcta de construir la cadena RAG en LCEL\n","# para poder acceder tanto a la respuesta como a las fuentes (contexto).\n","qa = (\n","    # 1. Define un diccionario paralelo para procesar la entrada.\n","    {\n","        # Clave 'context': Toma la entrada ('input'), la pasa (|) al 'retriever' y guarda los documentos resultantes.\n","        \"context\": itemgetter(\"input\") | retriever,\n","        # Clave 'input': Toma la entrada ('input') y la pasa directamente, sin cambios.\n","        \"input\": itemgetter(\"input\"),\n","    }\n","    # 2. Pasa el diccionario {'context', 'input'} a la siguiente etapa.\n","    | RunnablePassthrough.assign(\n","        # 3. Asigna una nueva clave ('answer') al diccionario.\n","        #    Para obtener su valor, pasa el diccionario al 'prompt' y luego el 'prompt' al 'llm'.\n","        answer=(prompt | llm)\n","    )\n",")\n","\n","# 4. Resultado final:\n","#    Al invocar 'qa', la salida serÃ¡ un diccionario completo:\n","#    {\n","#      'context': [Documentos recuperados],\n","#      'input': 'Pregunta del usuario',\n","#      'answer': <AIMessage con la respuesta del LLM>\n","#    }\n","\n","# Imprime la cadena para verificar su estructura\n","print(f\"{qa}\")"],"id":"qa-setup"},{"cell_type":"markdown","metadata":{"id":"gdidOAJkvBdi"},"source":["## 5) Consulta A. Ejemplo de **grounding** con fuentes\n","\n","Prueba con una pregunta del PDF, procura que exista informaciÃ³n sobre ella. La salida mostrarÃ¡ la respuesta del LLM y **los documentos fuente** (archivo y pÃ¡gina)."],"id":"gdidOAJkvBdi"},{"cell_type":"code","metadata":{"id":"ask","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762384979150,"user_tz":300,"elapsed":3742,"user":{"displayName":"Marcelo Sotaminga","userId":"12238593987105802968"}},"outputId":"dc12aaf6-6c89-4cfc-f01e-4da0ef53c679"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Respuesta similarity:\n"," La GeneraciÃ³n AutomÃ¡tica de Ãtems (AIG, por sus siglas en inglÃ©s) es un proceso que utiliza modelos de lenguaje avanzados para crear preguntas o Ã­tems de evaluaciÃ³n de manera eficiente y escalable. Este enfoque se basa en agentes autÃ³nomos que generan y revisan Ã­tems, asegurando objetividad y reduciendo sesgos. La AIG es crucial en la psicometrÃ­a y la evaluaciÃ³n de personal, optimizando la creaciÃ³n de pruebas y mejorando la calidad de los Ã­tems generados (Lee et al., 2025). \n","\n","Referencia: Lee, P. (2025). AI-powered Automatic Item Generation for Psychological Tests: A Conceptual Framework for an LLM-based Multi-Agent AIG System. *Journal of Business and Psychology*. https://doi.org/10.1007/s10869-025-10067-y\n","\n","ğŸ” Fuentes:\n","[1] planificacioÌn_curso-IA_Avanzado.pdf â€” pÃ¡gina 0\n","    â””â”€ \"guiada\n","Archivo .py o notebook con \n","ejemplos de consultas y \n","resultados comentados\n","29 de \n","octubre\n","4:00 p.m. - \n","7:00 p.m.\n","2. Asistentes \n","personalizados\n","Bot con RAG y \n","Chatbot con \n","FAQ \n","PrÃ¡ctica \n","guiada\n","Script o notebook co...\"\n","\n","[2] Lee et al. - 2025 - AI-powered Automatic Item Generation for Psychological Tests A Conceptual Framework for an LLM-base (1).pdf â€” pÃ¡gina 6\n","    â””â”€ \"1, the \n","proposed framework consists of autonomous, speciali zed \n","agents, each responsible for a distinct function wi thin the \n","item generation and review process. This system reï¬‚ects best \n","practices in test development b...\"\n","\n","[3] Lee et al. - 2025 - AI-powered Automatic Item Generation for Psychological Tests A Conceptual Framework for an LLM-base (1).pdf â€” pÃ¡gina 4\n","    â””â”€ \"spective manual review process could be time consumi ng \n","and labor intensive, particularly when applied to e xtensive \n","item pools, thereby limiting the scalability and ov erall eï¬ƒ -\n","ciency of AIG systems (Circi etÂ al., 2...\"\n","\n","[4] Lee et al. - 2025 - AI-powered Automatic Item Generation for Psychological Tests A Conceptual Framework for an LLM-base (1).pdf â€” pÃ¡gina 4\n","    â””â”€ \"developing the initial item pool (p. 1030)â€. Moreover, inte-\n","grating real-time grammatical validation could improve mul-\n","tilingual item generation, supporting the broader application \n","of LLM-based AIG across diverse ling...\"\n","\n"]}],"source":["\n","# Define una funciÃ³n 'ask' que recibe la pregunta del usuario ('query').\n","def ask(query: str):\n","\n","    # 1. Ejecuta la cadena 'qa' (definida antes) con la consulta del usuario.\n","    #    'out' es el diccionario: {'context': ..., 'input': ..., 'answer': ...}\n","    out = qa.invoke({'input': query})\n","\n","    # 2. Extrae la respuesta de texto (el .content) del objeto 'answer' (AIMessage).\n","    ans = out['answer'].content\n","\n","    # 3. Extrae la lista de documentos (fuentes) de la clave 'context'.\n","    srcs = out.get('context', [])\n","\n","    # 4. Imprime la respuesta principal generada por el LLM.\n","    print('Respuesta similarity:\\n', ans)\n","\n","    # Imprime el encabezado para la secciÃ³n de citas.\n","    print('\\nğŸ” Fuentes:')\n","\n","    # 5. Inicia un bucle (enumerado desde 1) por cada documento 'd' en las fuentes 'srcs'.\n","    for i, d in enumerate(srcs, 1):\n","\n","        # Obtiene los metadatos del documento (o un dict vacÃ­o si no hay).\n","        meta = d.metadata or {}\n","        # Obtiene el nombre del archivo ('source') de los metadatos.\n","        src = meta.get('source', 'desconocido')\n","        # Obtiene el nÃºmero de pÃ¡gina de los metadatos.\n","        page = meta.get('page', 'Â¿?')\n","\n","        # Crea un 'snippet' (fragmento) corto del contenido del documento.\n","        snippet = (d.page_content[:220] + '...') if len(d.page_content) > 220 else d.page_content\n","\n","        # Imprime la cita formateada (nÃºmero, archivo, pÃ¡gina y snippet).\n","        print(f'[{i}] {src} â€” pÃ¡gina {page}\\n    â””â”€ \"{snippet}\"\\n')\n","\n","# 6. Llama a la funciÃ³n 'ask' con una pregunta de prueba para ejecutar todo el flujo RAG.\n","ask('Â¿QuÃ© es la GeneraciÃ³n AutomÃ¡tica de Ã­tems?')"],"id":"ask"},{"cell_type":"markdown","metadata":{"id":"zagB5ZiIvBdi"},"source":["## 6) Ajustes de recuperaciÃ³n (k, search_type, filtros)\n","- **k:** nÃºmero de fragmentos recuperados.\n","- **search_type:** `'similarity'` o `'mmr'` (Maximum Marginal Relevance para mÃ¡s diversidad).\n","- **Filtros por metadatos:** si tus documentos tienen metadatos personalizados, se pueden pasar en `search_kwargs`."],"id":"zagB5ZiIvBdi"},{"cell_type":"code","metadata":{"id":"mmr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762385303022,"user_tz":300,"elapsed":3523,"user":{"displayName":"Marcelo Sotaminga","userId":"12238593987105802968"}},"outputId":"5d883068-170d-4886-9530-e13b73a5ea87"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Respuesta Relevancia Marginal MÃ¡xima (MMR):\n"," La GeneraciÃ³n AutomÃ¡tica de Ãtems (GAI) es un proceso que utiliza modelos de lenguaje, como los de inteligencia artificial, para crear preguntas y evaluaciones de manera eficiente. Este enfoque permite generar Ã­tems alineados con objetivos especÃ­ficos, mejorando la calidad y relevancia de las evaluaciones. Sin embargo, es crucial integrar la intervenciÃ³n humana para optimizar la calidad de los Ã­tems generados, evitando estancamientos en el proceso (Lee et al., 2025). \n","\n","Referencia: Lee, P. (2025). AI-powered Automatic Item Generation for Psychological Tests: A Conceptual Framework for an LLM-based Multi-Agent AIG System. *Journal of Business and Psychology*. https://doi.org/10.1007/s10869-025-10067-y\n","\n","ğŸ” Fuentes (MMR):\n","  [1] planificacioÌn_curso-IA_Avanzado.pdf (pÃ¡g. 0)\n","  [2] Lee et al. - 2025 - AI-powered Automatic Item Generation for Psychological Tests A Conceptual Framework for an LLM-base (1).pdf (pÃ¡g. 21)\n","  [3] Lee et al. - 2025 - AI-powered Automatic Item Generation for Psychological Tests A Conceptual Framework for an LLM-base (1).pdf (pÃ¡g. 16)\n","  [4] Lee et al. - 2025 - AI-powered Automatic Item Generation for Psychological Tests A Conceptual Framework for an LLM-base (1).pdf (pÃ¡g. 4)\n"]}],"source":["# Importa la clase que necesitamos para mantener el contexto.\n","from langchain_core.runnables import RunnablePassthrough\n","\n","\n","# Importa la clase para crear plantillas de prompts.\n","from langchain_core.prompts import ChatPromptTemplate\n","# Importa la clase para \"pasar\" datos (aunque no se usa en ESTA versiÃ³n de la cadena).\n","from langchain_core.runnables import RunnablePassthrough\n","# Importa la funciÃ³n para obtener claves de un diccionario.\n","from operator import itemgetter\n","\n","# 1. Crear un NUEVO retriever que usa 'MMR'\n","# (Asume que 'vectordb' ya existe en memoria).\n","retriever_mmr = vectordb.as_retriever(\n","    # 'mmr' (Maximum Marginal Relevance) busca chunks que sean relevantes Y diversos\n","    # (evitando que los 6 chunks digan lo mismo).\n","    search_type='mmr',\n","    search_kwargs={\n","        'k': 4,             # Le pedimos que recupere 6 chunks.\n","        'lambda_mult': 0.5  # Un balance (0.5) entre relevancia y diversidad.\n","        }\n",")\n","\n","# 2. Definir un NUEVO prompt, esta vez con reglas muy estrictas.\n","prompt = ChatPromptTemplate.from_template(\"\"\"Responde las preguntas de manera clara.\n"," La respuesta debe ser muy bien argumentada, empleando lenguaje tÃ©cnico de mÃ¡ximo 400 caracteres, agrega al menos una referencia a los documentos en formato APA7 y basada\n"," en este contexto:\n","<context>\n","{context}\n","</context>\n","\n","Question: {input}\"\"\")\n","\n","\n","# Construimos 'qa_mmr' usando el mismo patrÃ³n que 'qa' (para que devuelva las fuentes).\n","qa_mmr = (\n","    {\n","        # 1. Obtiene el contexto usando el retriever MMR.\n","        \"context\": itemgetter(\"input\") | retriever_mmr,\n","        # 2. Pasa la pregunta original.\n","        \"input\": itemgetter(\"input\"),\n","    }\n","    # 3. Mantiene el diccionario {'context', 'input'} Y le aÃ±ade una nueva clave: 'answer'.\n","    | RunnablePassthrough.assign(\n","        # 4. La clave 'answer' se genera pasando el diccionario al prompt y luego al LLM.\n","        answer=(prompt | llm)\n","    )\n",")\n","\n","# Ahora 'out' serÃ¡ un diccionario: {'context': ..., 'input': ..., 'answer': ...}\n","out = qa_mmr.invoke({'input': 'Â¿QuÃ© es la GeneraciÃ³n AutomÃ¡tica de Ã­tems?'})\n","\n","# Ahora puedes imprimir la respuesta Y las fuentes, igual que en tu funciÃ³n 'ask'\n","print(\"Respuesta Relevancia Marginal MÃ¡xima (MMR):\\n\", out['answer'].content)\n","print(\"\\nğŸ” Fuentes (MMR):\")\n","# Itera sobre los documentos de 'context' y los imprime.\n","for i, d in enumerate(out['context'], 1):\n","    print(f\"  [{i}] {d.metadata.get('source', '?')} (pÃ¡g. {d.metadata.get('page', '?')})\")"],"id":"mmr"},{"cell_type":"markdown","metadata":{"id":"1BrL-xmTvBdi"},"source":["## 7) Interfaz rÃ¡pida con Gradio\n","AquÃ­ una interfaz sencilla para probar preguntas y ver las **citas** a las fuentes."],"id":"1BrL-xmTvBdi"},{"cell_type":"code","metadata":{"id":"gradio","colab":{"base_uri":"https://localhost:8080/","height":670},"executionInfo":{"status":"ok","timestamp":1762386575642,"user_tz":300,"elapsed":1471,"user":{"displayName":"Marcelo Sotaminga","userId":"12238593987105802968"}},"outputId":"6fa77212-8685-473e-cbde-af9ab915c94d"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n","  self.chatbot = Chatbot(\n"]},{"output_type":"stream","name":"stdout","text":["Lanzando interfaz de Gradio...\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n","* To create a public link, set `share=True` in `launch()`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["(async (port, path, width, height, cache, element) => {\n","                        if (!google.colab.kernel.accessAllowed && !cache) {\n","                            return;\n","                        }\n","                        element.appendChild(document.createTextNode(''));\n","                        const url = await google.colab.kernel.proxyPort(port, {cache});\n","\n","                        const external_link = document.createElement('div');\n","                        external_link.innerHTML = `\n","                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n","                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n","                                    https://localhost:${port}${path}\n","                                </a>\n","                            </div>\n","                        `;\n","                        element.appendChild(external_link);\n","\n","                        const iframe = document.createElement('iframe');\n","                        iframe.src = new URL(path, url).toString();\n","                        iframe.height = height;\n","                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n","                        iframe.width = width;\n","                        iframe.style.border = 0;\n","                        element.appendChild(iframe);\n","                    })(7861, \"/\", \"100%\", 500, false, window.element)"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":32}],"source":["# --- 1. Importar todas las bibliotecas necesarias ---\n","# (AsegÃºrate de haberlas instalado con !pip install...)\n","import gradio as gr\n","import os\n","from operator import itemgetter\n","from langchain_community.document_loaders import PyPDFLoader\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain_community.vectorstores import Chroma\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.runnables import RunnablePassthrough\n","\n","\n","# --- 2. ConfiguraciÃ³n Global (Modelos) ---\n","\n","# Carga el modelo de embedding gratuito\n","model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n","embeddings = HuggingFaceEmbeddings(model_name=model_name)\n","\n","# Carga el LLM (asume que la API key de OpenAI estÃ¡ en el entorno)\n","llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n","\n","# Plantilla de Prompt (usaremos la misma para ambos RAGs para una comparaciÃ³n justa)\n","prompt = ChatPromptTemplate.from_template(\"\"\"Responde la pregunta de forma concisa (mÃ¡ximo 3 frases), usa APA7 y una referencia textual basÃ¡ndote solo en este contexto:\n","<context>\n","{context}\n","</context>\n","\n","Pregunta: {input}\"\"\")\n","\n","\n","# --- 3. FunciÃ³n de IndexaciÃ³n (El \"Paso 1\") ---\n","# Esta funciÃ³n AHORA CREA AMBAS CADENAS RAG\n","\n","def process_files(file_list):\n","    print(\"Iniciando procesamiento de archivos...\")\n","    if not file_list:\n","        return None, \"âŒ Error: No se subieron archivos.\"\n","\n","    all_chunks = []\n","\n","    # 1. Cargar y Dividir los PDFs\n","    try:\n","        for file_obj in file_list:\n","            print(f\"  Cargando: {os.path.basename(file_obj.name)}\")\n","            loader = PyPDFLoader(file_obj.name)\n","            docs = loader.load()\n","\n","            splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=120)\n","            chunks = splitter.split_documents(docs)\n","            all_chunks.extend(chunks)\n","\n","        print(f\"Total de chunks generados: {len(all_chunks)}\")\n","\n","        # 2. Crear la Base de Datos Vectorial (en memoria)\n","        print(\"Creando base de datos vectorial (ChromaDB)...\")\n","        vectordb = Chroma.from_documents(\n","            documents=all_chunks,\n","            embedding=embeddings\n","        )\n","\n","        # 3. Crear el Pipeline RAG (Similarity)\n","        retriever_sim = vectordb.as_retriever(search_type='similarity', search_kwargs={'k': 4})\n","        qa_sim = (\n","            {\"context\": itemgetter(\"input\") | retriever_sim, \"input\": itemgetter(\"input\")}\n","            | RunnablePassthrough.assign(answer=(prompt | llm))\n","        )\n","        print(\"âœ… Pipeline RAG (Similarity) listo.\")\n","\n","        # 4. Crear el Pipeline RAG (MMR)\n","        retriever_mmr = vectordb.as_retriever(search_type='mmr', search_kwargs={'k': 4, 'lambda_mult': 0.5})\n","        qa_mmr = (\n","            {\"context\": itemgetter(\"input\") | retriever_mmr, \"input\": itemgetter(\"input\")}\n","            | RunnablePassthrough.assign(answer=(prompt | llm))\n","        )\n","        print(\"âœ… Pipeline RAG (MMR) listo.\")\n","\n","        # 5. Guardar AMBAS cadenas en un diccionario\n","        all_chains = {\n","            \"RAG (Similarity)\": qa_sim,\n","            \"RAG (MMR)\": qa_mmr\n","        }\n","\n","        # Devuelve el diccionario de cadenas Y un mensaje de Ã©xito\n","        return all_chains, f\"âœ… Â¡Ã‰xito! {len(all_chunks)} chunks procesados. Cadenas 'Similarity' y 'MMR' creadas.\"\n","\n","    except Exception as e:\n","        print(f\"Error procesando archivos: {e}\")\n","        return None, f\"âŒ Error: {e}\"\n","\n","\n","# --- 4. FunciÃ³n de Chat (El \"Paso 2\") ---\n","# Esta funciÃ³n AHORA ACEPTA EL TIPO DE RAG\n","\n","def rag_chat(user_message, history, rag_chain_state, rag_type):\n","\n","    # Comprueba si el pipeline RAG (\"state\") ha sido creado\n","    if rag_chain_state is None:\n","        return \"Por favor, sube uno o mÃ¡s archivos PDF y presiona 'Procesar Archivos' primero.\"\n","\n","    # Usa el 'rag_type' para seleccionar la cadena correcta del diccionario 'rag_chain_state'\n","    try:\n","        # Elige la cadena correcta del diccionario guardado en el estado\n","        chain_to_use = rag_chain_state[rag_type]\n","        print(f\"Ejecutando con: {rag_type}\")\n","\n","        # Invoca la cadena seleccionada\n","        out = chain_to_use.invoke({'input': user_message})\n","\n","        # Extrae la respuesta y las fuentes\n","        ans = out['answer'].content\n","        srcs = out.get('context', [])\n","\n","        # Formatea las citas\n","        cites = []\n","        for d in srcs:\n","            meta = d.metadata or {}\n","            src = os.path.basename(meta.get('source','?')) # Solo el nombre del archivo\n","            page = meta.get('page','?')\n","            cites.append(f\"{src} (pÃ¡g. {page})\")\n","\n","        footer = '\\n\\nFuentes:\\n- ' + '\\n- '.join(dict.fromkeys(cites)) if cites else ''\n","        return ans + footer\n","\n","    except Exception as e:\n","        print(f\"Error en 'rag_chat': {e}\")\n","        return f\"Error al generar respuesta: {e}\"\n","\n","\n","# --- 5. Construir la Interfaz de Gradio con \"Blocks\" ---\n","\n","with gr.Blocks(title=\"Chatbot RAG con Carga y Selector\") as demo:\n","\n","    # 'gr.State' es la variable \"invisible\" que guarda el diccionario de pipelines RAG\n","    rag_chain_state = gr.State(None)\n","\n","    gr.Markdown(\"# Chatbot RAG: Carga de Archivos y Comparador (Similarity vs. MMR)\")\n","    gr.Markdown(\"Sube tus PDFs, presiona 'Procesar Archivos' y luego chatea con tus documentos.\")\n","\n","    with gr.Row():\n","        with gr.Column(scale=1):\n","            # Componente de carga de archivos\n","            file_uploader = gr.File(\n","                label=\"Sube tus PDFs\",\n","                file_count=\"multiple\",\n","                file_types=[\".pdf\"]\n","            )\n","\n","            # Selector de tipo de RAG\n","            rag_selector = gr.Radio(\n","                [\"RAG (Similarity)\", \"RAG (MMR)\"],\n","                label=\"Elige el tipo de Retriever\",\n","                value=\"RAG (Similarity)\" # Valor por defecto\n","            )\n","\n","            # BotÃ³n para iniciar la indexaciÃ³n\n","            process_button = gr.Button(\"Procesar Archivos\", variant=\"primary\")\n","\n","            # Caja de estado para mensajes de Ã©xito o error\n","            status_box = gr.Textbox(label=\"Estado\", interactive=False)\n","\n","        with gr.Column(scale=2):\n","            # Interfaz de chat estÃ¡ndar\n","            chat_interface = gr.ChatInterface(\n","                fn=rag_chat,\n","                # Pasa AMBAS entradas adicionales a la funciÃ³n de chat\n","                additional_inputs=[rag_chain_state, rag_selector]\n","            )\n","\n","    # --- 6. Conectar los Componentes ---\n","\n","    # Conecta el botÃ³n 'process_button' a la funciÃ³n 'process_files'\n","    process_button.click(\n","        fn=process_files,\n","        inputs=[file_uploader],\n","        outputs=[rag_chain_state, status_box] # La salida se guarda en el estado y el status\n","    )\n","\n","print(\"Lanzando interfaz de Gradio...\")\n","demo.launch(share=False)"],"id":"gradio"},{"cell_type":"markdown","metadata":{"id":"QEesKSn1vBdi"},"source":["## 8) Buenas prÃ¡cticas y notas finales\n","- **CitaciÃ³n obligatoria**: siempre devolver fuentes (archivo y pÃ¡gina) para confianza y auditorÃ­a.\n","- **EvaluaciÃ³n**: verifica que los fragmentos recuperados contienen evidencia suficiente.\n","- **TamaÃ±o de chunk**: prueba 400â€“900 tokens; solape 10â€“20%.\n","- **k y search_type**: ajusta segÃºn el corpus. `mmr` puede mejorar diversidad.\n","- **Persistencia**: usa `persist_directory` para reutilizar Ã­ndices.\n","- **Privacidad**: no subas documentos sensibles a Colab sin autorizaciÃ³n."],"id":"QEesKSn1vBdi"}],"metadata":{"colab":{"provenance":[{"file_id":"1mI0hIVc75UaEiRSF21YjAYPEHkL3mpu1","timestamp":1762020104439}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10"}},"nbformat":4,"nbformat_minor":5}